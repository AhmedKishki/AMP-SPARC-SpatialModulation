{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPARC AMP Example\n",
    "\n",
    "This notebook contains a complete worked example of a SPARC AMP encoder/decoder using Hadamard design matrices and various power allocation strategies.\n",
    "\n",
    "For full details of the algorithm please see our paper _\"Capacity-achieving Sparse Superposition Codes via Approximate Message Passing Decoding_\": https://arxiv.org/abs/1501.05892\n",
    "\n",
    "For further optimisation details please see the follow up paper _\"Optimising the Finite-Length Performance of AMP-decoded Sparse Regression Codes\"_.\n",
    "\n",
    "This notebook is Copyright 2016, 2017 Adam Greig."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "We need numpy and ideally an accelerated Hadamard transform written in C, available to install as the `pyfht` library (try `sudo pip install pyfht` or similar for your Python package installation system).\n",
    "\n",
    "If `pyfht` is not available we can fall back to a Python version, but beware it is a great deal slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    from pyfht import fht_inplace\n",
    "except ImportError:\n",
    "    import warnings\n",
    "    warnings.warn(\"Using very slow Python version of fht, please install pyfht\")\n",
    "    def fht_inplace(x):\n",
    "        N = len(x)\n",
    "        i = N>>1\n",
    "        while i:\n",
    "            for k in range(0, N, 2*i):\n",
    "                for j in range(k, i+k):\n",
    "                    ij = i|j\n",
    "                    temp = x[j]\n",
    "                    x[j] += x[ij]\n",
    "                    x[ij] = temp - x[ij]\n",
    "            i = i >> 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fast Hadamard Transforms\n",
    "\n",
    "These functions use the in-place Hadamard transform to emulate matrix operations on a random $\\pm1$ matrix of any size smaller than the underlying Hadamard matrix. We then also combine $L$ such operations for efficiency when using very wide matrices, typical for SPARC dictionaries.\n",
    "\n",
    "None of this code is particularly relevant to the actual SPARC-AMP decoder, so feel free to skip over it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sub_fht(n, m, seed=0, ordering=None):\n",
    "    \"\"\"\n",
    "    Returns functions to compute the sub-sampled Walsh-Hadamard transform,\n",
    "    i.e., operating with a wide rectangular matrix of random +/-1 entries.\n",
    "\n",
    "    n: number of rows\n",
    "    m: number of columns\n",
    "\n",
    "    It is most efficient (but not required) for max(m+1,n+1) to be a power of 2.\n",
    "\n",
    "    seed: determines choice of random matrix\n",
    "    ordering: optional n-long array of row indices in [1, max(m,n)] to\n",
    "              implement subsampling; generated by seed if not specified,\n",
    "              but may be given to speed up subsequent runs on the same matrix.\n",
    "\n",
    "    Returns (Ax, Ay, ordering):\n",
    "        Ax(x): computes A.x (of length n), with x having length m\n",
    "        Ay(y): computes A'.y (of length m), with y having length n\n",
    "        ordering: the ordering in use, which may have been generated from seed\n",
    "    \"\"\"\n",
    "    assert n > 0, \"n must be positive\"\n",
    "    assert m > 0, \"m must be positive\"\n",
    "    w = 2**int(np.ceil(np.log2(max(m+1, n+1))))\n",
    "\n",
    "    if ordering is not None:\n",
    "        assert ordering.shape == (n,)\n",
    "    else:\n",
    "        rng = np.random.RandomState(seed)\n",
    "        idxs = np.arange(1, w, dtype=np.uint32)\n",
    "        rng.shuffle(idxs)\n",
    "        ordering = idxs[:n]\n",
    "\n",
    "    def Ax(x):\n",
    "        assert x.size == m, \"x must be m long\"\n",
    "        y = np.zeros(w)\n",
    "        y[w-m:] = x.reshape(m)\n",
    "        fht_inplace(y)\n",
    "        return y[ordering]\n",
    "\n",
    "    def Ay(y):\n",
    "        assert y.size == n, \"input must be n long\"\n",
    "        x = np.zeros(w)\n",
    "        x[ordering] = y\n",
    "        fht_inplace(x)\n",
    "        return x[w-m:]\n",
    "\n",
    "    return Ax, Ay, ordering\n",
    "\n",
    "def block_sub_fht(n, m, l, seed=0, ordering=None):\n",
    "    \"\"\"\n",
    "    As `sub_fht`, but computes in `l` blocks of size `n` by `m`, potentially\n",
    "    offering substantial speed improvements.\n",
    "\n",
    "    n: number of rows\n",
    "    m: number of columns per block\n",
    "    l: number of blocks\n",
    "\n",
    "    It is most efficient (though not required) when max(m+1,n+1) is a power of 2.\n",
    "\n",
    "    seed: determines choice of random matrix\n",
    "    ordering: optional (l, n) shaped array of row indices in [1, max(m, n)] to\n",
    "              implement subsampling; generated by seed if not specified, but\n",
    "              may be given to speed up subsequent runs on the same matrix.\n",
    "\n",
    "    Returns (Ax, Ay, ordering):\n",
    "        Ax(x): computes A.x (of length n), with x having length l*m\n",
    "        Ay(y): computes A'.y (of length l*m), with y having length n\n",
    "        ordering: the ordering in use, which may have been generated from seed\n",
    "    \"\"\"\n",
    "    assert n > 0, \"n must be positive\"\n",
    "    assert m > 0, \"m must be positive\"\n",
    "    assert l > 0, \"l must be positive\"\n",
    "\n",
    "    if ordering is not None:\n",
    "        assert ordering.shape == (l, n)\n",
    "    else:\n",
    "        w = 2**int(np.ceil(np.log2(max(m+1, n+1))))\n",
    "        rng = np.random.RandomState(seed)\n",
    "        ordering = np.empty((l, n), dtype=np.uint32)\n",
    "        idxs = np.arange(1, w, dtype=np.uint32)\n",
    "        for ll in range(l):\n",
    "            rng.shuffle(idxs)\n",
    "            ordering[ll] = idxs[:n]\n",
    "\n",
    "    def Ax(x):\n",
    "        assert x.size == l*m\n",
    "        out = np.zeros(n)\n",
    "        for ll in range(l):\n",
    "            ax, ay, _ = sub_fht(n, m, ordering=ordering[ll])\n",
    "            out += ax(x[ll*m:(ll+1)*m])\n",
    "        return out\n",
    "\n",
    "    def Ay(y):\n",
    "        assert y.size == n\n",
    "        out = np.empty(l*m)\n",
    "        for ll in range(l):\n",
    "            ax, ay, _ = sub_fht(n, m, ordering=ordering[ll])\n",
    "            out[ll*m:(ll+1)*m] = ay(y)\n",
    "        return out\n",
    "\n",
    "    return Ax, Ay, ordering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPARC Dictionary\n",
    "\n",
    "We use the `block_sub_fht` which computes the equivalent of $A\\beta$ by using $L$ separate $M\\times M$ Hadamard matrices. However we want each entry to be divided by $\\sqrt{n}$ to get the right variance, and we need to do a reshape on the output to get column vectors, so we'll wrap those operations here.\n",
    "\n",
    "Returns two functions `Ab` and `Az` which compute $A\\beta$ and $A^Tz$ respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sparc_transforms(L, M, n):\n",
    "    Ax, Ay, _ = block_sub_fht(n, M, L, ordering=None)\n",
    "    def Ab(b):\n",
    "        return Ax(b).reshape(-1, 1) / np.sqrt(n)\n",
    "    def Az(z):\n",
    "        return Ay(z).reshape(-1, 1) / np.sqrt(n)\n",
    "    return Ab, Az"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Power Allocation\n",
    "\n",
    "There are multiple choices for power allocation algorithm. Here we give the \"standard exponential\" allocation used for the capacity-achieving proof; a similar exponential with parameterised steepness which is flattened after some point; and a newer iterative allocation which aims to allocate just sufficient power for blocks of sections to decode successively. You can select which is used by altering the function called from `amp_sim` below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original\n",
    "This power allocation is:\n",
    "$P_\\ell \\propto 2^{-2\\mathcal{C}\\ell/L}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pa_original(L, C, P):\n",
    "    pa = 2**(-2 * a * C * np.arange(L) / L)\n",
    "    pa /= pa.sum() / P\n",
    "    return pa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameterised\n",
    "\n",
    "We generate a power allocation parameterised by $a$ and $f$ which set the steepness and the flattening point respectively.\n",
    "\n",
    "The power allocation follows $P_\\ell \\propto 2^{-2a\\mathcal{C}\\ell/L}$ up to $fL$, and then is flat after that point, and scaled so that the sum power is $P$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pa_parameterised(L, C, P, a, f):\n",
    "    pa = 2**(-2 * a * C * np.arange(L) / L)\n",
    "    pa[int(f*L):] = pa[int(f*L)]\n",
    "    pa /= pa.sum() / P\n",
    "    return pa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative\n",
    "\n",
    "We split the sections into $B$ blocks of $L/B$ sections each, where each section in a block has the same power. Then each block is assigned just enough power to meet the requirement $P_\\ell > 2(\\ln 2)R\\tau_t^2 / L$. In practice the rate $R_\\text{PA}$ used here is set independently from the actual codeword rate to allow altering the power allocation. When the total unallocated power is sufficient to give all remaining sections the same power, we do that to complete the allocation.\n",
    "\n",
    "One modification is to pass $B=L$, so each block contains only one section. This tends to perform better when given sufficient iterations.\n",
    "\n",
    "For further details on this power allocation, see _\"Optimising the Finite-Length Performance of AMP-decoded Sparse Regression Codes\"_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pa_iterative(L, B, σ, P, R_PA):\n",
    "    PA = np.zeros(L)\n",
    "    τ = np.zeros(B)\n",
    "    k = L//B\n",
    "    for b in range(B):\n",
    "        Premain = P - PA.sum()\n",
    "        τ[b] = np.sqrt(σ**2 + Premain)\n",
    "        Pblock = 2 * np.log(2) * (R_PA/L) * τ[b]**2\n",
    "        Pspread = Premain / (L - k*b)\n",
    "        if Pblock > Pspread:\n",
    "            PA[k*b:k*(b+1)] = Pblock\n",
    "        else:\n",
    "            PA[k*b:] = Pspread\n",
    "            break\n",
    "    return PA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AMP\n",
    "This is the actual AMP algorithm. It's a mostly straightforward transcription from the relevant equations, but we regularise the exponential terms to prevent overflow and improve numerical precision.\n",
    "\n",
    "We estimate $\\tau^2$ from $\\mathrm{Var}[z]$ rather than pre-computed constants.\n",
    "If $\\tau^2$ hasn't changed between two iterations, we can stop early.\n",
    "For further details on these optimisations, see _\"Optimising the Finite-Length Performance of AMP-decoded Sparse Regression Codes\"_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def amp(y, σ_n, Pl, L, M, T, Ab, Az):\n",
    "    P = np.sum(Pl)\n",
    "    n = y.size\n",
    "    β = np.zeros((L*M, 1))\n",
    "    z = y\n",
    "    last_τ = 0\n",
    "    \n",
    "    for t in range(T):\n",
    "        τ = np.sqrt(np.sum(z**2)/n)\n",
    "        if τ == last_τ:\n",
    "            return β\n",
    "        last_τ = τ\n",
    "        \n",
    "        s = β + Az(z)\n",
    "        rt_n_Pl = np.sqrt(n*Pl).repeat(M).reshape(-1, 1)\n",
    "        u = s * rt_n_Pl / τ**2\n",
    "        max_u = u.max()\n",
    "        exps = np.exp(u - max_u)\n",
    "        sums = exps.reshape(L, M).sum(axis=1).repeat(M).reshape(-1, 1)\n",
    "        β = (rt_n_Pl * exps / sums).reshape(-1, 1)\n",
    "        z = y - Ab(β) + (z/τ**2) * (P - np.sum(β**2) / n)\n",
    "    \n",
    "    return β"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPARC-AMP Simulation\n",
    "Here we perform the actual simulation. The comments inline guide you through what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def amp_sim(L, M, σ_n, P, R, T, R_PA):\n",
    "    # Compute the SNR, capacity, and n, from the input parameters\n",
    "    snr = P / σ_n**2\n",
    "    C = 0.5 * np.log2(1 + snr)\n",
    "    n = int(L*np.log2(M) / R)\n",
    "    \n",
    "    # Generate the power allocation\n",
    "    Pl = pa_iterative(L, L, σ_n, P, R_PA)\n",
    "    \n",
    "    # Generate random message in [0..M)^L\n",
    "    tx_message = np.random.randint(0, M, L).tolist()\n",
    "    \n",
    "    # Generate the SPARC transform functions A.beta and A'.z\n",
    "    Ab, Az = sparc_transforms(L, M, n)\n",
    "    \n",
    "    # Generate our transmitted signal X\n",
    "    β_0 = np.zeros((L*M, 1))\n",
    "    for l in range(L):\n",
    "        β_0[l*M + tx_message[l]] = np.sqrt(n * Pl[l])\n",
    "    x = Ab(β_0)\n",
    "    \n",
    "    # Generate random channel noise and then received signal y\n",
    "    z = np.random.randn(n, 1) * σ_n\n",
    "    y = (x + z).reshape(-1, 1)\n",
    "        \n",
    "    # Run AMP decoding\n",
    "    β = amp(y, σ_n, Pl, L, M, T, Ab, Az).reshape(-1)\n",
    "    \n",
    "    # Convert decoded beta back to a message\n",
    "    rx_message = []\n",
    "    for l in range(L):\n",
    "        idx = np.argmax(β[l*M:(l+1)*M])\n",
    "        rx_message.append(idx)\n",
    "    \n",
    "    # Compute fraction of sections decoded correctly\n",
    "    correct = np.sum(np.array(rx_message) == np.array(tx_message)) / L\n",
    "    \n",
    "    # Compute BER\n",
    "    ber = np.sum(bin(a^b).count('1')\n",
    "                 for (a, b) in zip(tx_message, rx_message))/(L*np.log2(M))\n",
    "    \n",
    "    # Compute Eb/N0\n",
    "    EbN0 = 1/(2*R) * (P/σ_n**2)\n",
    "    \n",
    "    return {\n",
    "        \"L\": L, \"M\": M, \"sigma_n\": σ_n, \"P\": P, \"R\": R, \"T\": T,\n",
    "        \"snr\": snr, \"C\": C, \"n\": n, \"fc\": correct, \"R_PA\": R_PA,\n",
    "        \"ber\": ber, \"EbN0\": EbN0, \"ser\": 1-correct\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a trial run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 2.0,\n",
       " 'EbN0': 5.357142857142858,\n",
       " 'L': 1024,\n",
       " 'M': 512,\n",
       " 'P': 15.0,\n",
       " 'R': 1.4,\n",
       " 'R_PA': 1.4,\n",
       " 'T': 64,\n",
       " 'ber': 0.0,\n",
       " 'fc': 1.0,\n",
       " 'n': 6582,\n",
       " 'ser': 0.0,\n",
       " 'sigma_n': 1.0,\n",
       " 'snr': 15.0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "amp_sim(1024, 512, 1.0, 15.0, 1.4, 64, 1.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fc` is the fraction of sections decoded correctly - 100% in this case, so no errors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_threshold": 6,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
